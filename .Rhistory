nrow(dt.female.matched) / length(matched.users.ids)
# calculating number of males
dt.male.matched <- dt.users.matched[gender == "Male", ]
nrow(dt.male.matched)
# the two number are really close, with a small lead in matches number for females (51,6%)
# calculating number of matches per user id
dt.matches.per.user <- as.data.frame(table(dt.matched.users$matched_users_ids))
names(dt.matches.per.user) <- c("user_id", "Freq")
dt.matches.per.user$user_id <- as.numeric(dt.matches.per.user$user_id)
# merging the database with the females only by user_id, so that I will have the frequences as well
dt.female.matched.expanded <- merge(dt.female.matched,
dt.matches.per.user, by = c("user_id"))
# summing
sum(dt.female.matched.expanded$Freq)
# average number of matches per female user
sum(dt.female.matched.expanded$Freq) / nrow(dt.female.matched)
# an average of 63 matches per female user
# Same process for male users
# merging the database with the males only by user_id, so that I will have the frequences as well
dt.male.matched.expanded <- merge(dt.male.matched,
dt.matches.per.user, by = c("user_id"))
# summing
sum(dt.male.matched.expanded$Freq)
# average number of matches per male user
sum(dt.male.matched.expanded$Freq) / nrow(dt.male.matched)
# an average of 63 matches per male user, there is almost no difference. This could be expected since, as we saw, the number of same-sex likes (and thus matches) is really similar
# checking the median
median(dt.matches.per.user$Freq)
ggplot(dt.matches.per.user, aes(x = Freq)) + geom_histogram() +
geom_vline(aes(xintercept=median(Freq)),
color="red", linetype="dashed", size=1)
# the distribution of matches looks more like an in-between (aka mixed) network. There is not a thick and heavy tail, yet is presented and very long. The lenght of the tail, as well as the postion of the peak refutes the random distribution shape. A possible reason of this is the goal that a user of this platform has. While someone could use it just to increase his ego (see any of the Tinder documentaries on netflix), most of the users will stop using it as soon as they have a match with someone they like, since they will, most likely, start a relationship with them. Therefore, although not imposed, the platform has a natural limit of matches which leads to a very similar dynamic than the one present in FB with the friend size limit. This is confirmed also by the median being very close to the most users cluster
# calculating year that would shows whether someone is over of under 27
2018-27
# matches
# creating a subset with only over 27
dt.users.over <- dt.users[birth_year < 1991, ]
#checking number of users
nrow(dt.users.over)
# same as done for males and females databases, basically adding frequency
dt.users.over.expanded <- merge(dt.users.over,
dt.matches.per.user, by=c("user_id"))
# summing
sum(dt.users.over.expanded$Freq)
# average number of matches per over 27
sum(dt.users.over.expanded$Freq) / nrow(dt.users.over)
# creating a subset with only under 27
dt.users.under <- dt.users[birth_year >= 1991, ]
# same as done for males and females databases, basically adding frequency
dt.users.under.expanded <- merge(dt.users.under,
dt.matches.per.user, by=c("user_id"))
# summing
sum(dt.users.under.expanded$Freq)
# average number of matches per under 27
sum(dt.users.under.expanded$Freq) / nrow(dt.users.under)
# the number of matches per user is about twice as big for over 27 users compared to under 27. This can be due to two reasons. Firstly due tothe platform population being mostly over 27, thus matches are probably easier. Secondly due to the fact that, given the payment, they knwo who visited the profile and, most likely, left a like. Therefore, matches are more likely.
# likes
# creating a subset with only over 27
dt.over.likes <- dt.likes[age_sender > 27, ]
# number of likes sent by an over 27 user
nrow(dt.over.likes)
# number of likes per over 27 user
nrow(dt.over.likes) / nrow(dt.users.over)
# creating a subset with only under 27
dt.under.likes <- dt.likes[age_sender <= 27, ]
# number of likes sent by an over 27 user
nrow(dt.under.likes)
# number of likes per over 27 user
nrow(dt.under.likes) / nrow(dt.users.under)
# the same pattern than the one saw for the matches can be seen for the likes as well
# <Answer here>
View(dt.homosexual.like)
# counting female same sex likes
nrow(dt.homosexual.like[gender_sender == "Female"])
# males have a higher number of same sex likes
# removing same auto likes
dt.homosexual.like[sender_user_id == receiver_user_id]
# removing same auto likes
nrow(dt.homosexual.like[sender_user_id == receiver_user_id])
# checking for "auto likes"
nrow(dt.homosexual.like[sender_user_id == receiver_user_id])
#336 likes were between same ids.
#removing these lines
dt.homosexual.like <- dt.homosexual.like[!(sender_user_id == receiver_user_id), ]
#re-checking length
nrow(dt.homosexual.like)
# counting males same sex likes
nrow(dt.homosexual.like[gender_sender == "Male"])
# counting female same sex likes
nrow(dt.homosexual.like[gender_sender == "Female"])
# males have a higher number of same sex likes
View(dt.matches.per.user)
#checking graphically the above observations
dt.likes.per.year <- dt.likes[, list(n_likes = .N), by="age_sender"]
View(dt.likes.per.year)
dt.likes.per.year$n_users <- dt.likes[, length(unique(user_id)), by="age_sender"]
dt.likes.per.year$n_users <- dt.likes[, unique(user_id), by="age_sender"]
dt.likes.per.year$n_users <- dt.likes[, length(unique(sender_user_id)),
by="age_sender"]
dt.likes.per.year$n_users <- dt.likes[, unique(sender_user_id),
by="age_sender"]
length(
dt.likes.per.year$n_users <- dt.likes[, length(unique(sender_user_id)),
by="age_sender"]
dt.likes.per.year$n_users <- dt.likes[, length(unique(sender_user_id)),
dt.likes.per.year$n_users <- dt.likes[, length(unique(sender_user_id)), by="age_sender"]
dt.users.per.year <- dt.likes[, list(m_users = unique(sender_user_id)), by="age_sender"]
#merging databases by age sender
dt.per.year<- merge(dt.likes.per.year, dt.users.per.year, by=c("age_sender"))
View(dt.per.year)
#creating a database with number of users per age
dt.users.per.year <- dt.likes[, list(n_users = unique(sender_user_id)), by="age_sender"]
View(dt.users.per.year)
#merging databases by age sender
dt.per.year<- merge(dt.likes.per.year, dt.users.per.year, by=c("age_sender"))
#creating a database with number of likes per age
dt.likes.per.year <- dt.likes[, list(n_likes = .N), by="age_sender"]
#creating a database with number of users per age
dt.users.per.year <- dt.likes[, list(n_users = unique(sender_user_id)), by="age_sender"]
#merging databases by age sender
dt.per.year<- merge(dt.likes.per.year, dt.users.per.year, by=c("age_sender"))
#creating a database with number of likes per age
dt.likes.per.year <- dt.likes[, list(n_likes = .N), by="age_sender"]
View(dt.likes.per.year)
#creating a database with number of users per age
dt.users.per.year <- dt.likes[, list(n_users = unique(sender_user_id)), by="age_sender"]
View(dt.users.per.year)
#creating a database with number of users per age
dt.users.per.year <- dt.likes[, list(n_users = length(unique(sender_user_id))), by="age_sender"]
sum(dt.users.per.year$n_users)
#calculating average per year
dt.per.year$average_n_likes <- dt.per.year$n_likes / dt.per.year$n_users
#creating a database with number of likes per age
dt.likes.per.year <- dt.likes[, list(n_likes = .N), by="age_sender"]
#creating a database with active number of users per age
dt.users.per.year <- dt.likes[, list(n_users = length(unique(sender_user_id))), by="age_sender"]
#calculating average per year
dt.per.year$average_n_likes <- dt.per.year$n_likes / dt.per.year$n_users
#creating a database with active number of users per age
dt.users.per.year <- dt.likes[, list(n_users = length(unique(sender_user_id))), by="age_sender"]
#creating a database with number of likes per age
dt.likes.per.year <- dt.likes[, list(n_likes = .N), by="age_sender"]
#merging databases by age sender
dt.per.year<- merge(dt.likes.per.year, dt.users.per.year, by=c("age_sender"))
#calculating average per year
dt.per.year$average_n_likes <- dt.per.year$n_likes / dt.per.year$n_users
#plotting
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) + geom_point()
# average number of likes per over 27 user
nrow(dt.under.likes) / nrow(dt.users.under)
# the same pattern than the one saw for the matches can be seen for the likes as well
#checking graphically the above observations
#creating a database with number of likes per age
dt.likes.per.year <- dt.likes[, list(n_likes = .N), by="age_sender"]
#creating a database with active number of users per age
dt.users.per.year <- dt.likes[, list(n_users = length(unique(sender_user_id))), by="age_sender"]
#merging databases by age sender
dt.per.year<- merge(dt.likes.per.year, dt.users.per.year, by=c("age_sender"))
#calculating average per year
dt.per.year$average_n_likes <- dt.per.year$n_likes / dt.per.year$n_users
#plotting
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) + geom_point()
#except from the outliers, we can see how the number on likes drastically increasese with the age increase, up to  55 y/o, then it dramatically decreases. It is thus confirmed that the number of likes is higher for paying users.
View(dt.matches)
View(dt.matched.users)
View(dt.users.matched)
View(dt.matches.per.user)
# average number of matches per male user
sum(dt.male.matched.expanded$Freq) / nrow(dt.male.matched)
# an average of 63 matches per male user, there is almost no difference. This could be expected since, as we saw, the number of same-sex likes (and thus matches) is really similar
# summing
males.matches <- sum(dt.male.matched.expanded$Freq)
males.matches
# average number of matches per male user
males.matches / nrow(dt.male.matched)
# summing
female.matches <- sum(dt.female.matched.expanded$Freq)
female.matches
# average number of matches per female user
female.matches / nrow(dt.female.matched)
# doublechecking for the number of matches
males.matches + female.matches == nrow(dt.matches)
# doublechecking for the number of matches
males.matches + female.matches == 2 * nrow(dt.matches)
dt.users.expanded <- merge(dt.users, dt.matches.per.user, by = c("user_id"))
View(dt.users.expanded)
#changing birth year to age
dt.users.expanded$age <- 2018 - dt.users.expanded$birth_year
#collapsing by year
dt.matches.per.year <- dt.users.expanded[, sum(Freq), by="age"]
View(dt.matches.per.year)
#collapsing by year
dt.matches.per.year <- dt.users.expanded[, list(n_matches = sum(Freq)), by = "age"]
#plotting
ggplot(dt.users.expanded, aes(x = age, y = n_matches)) + geom_point()
#plotting
ggplot(dt.matches.per.year, aes(x = age, y = n_matches)) + geom_point()
# checking graphically the above observations
# creating a database with number of likes per age
dt.likes.per.year <- dt.likes[, list(n_likes = .N), by="age_sender"]
# creating a database with active number of users per age
dt.users.per.year <- dt.likes[, list(n_users = length(unique(sender_user_id))), by="age_sender"]
# merging databases by age sender
dt.per.year<- merge(dt.likes.per.year, dt.users.per.year, by=c("age_sender"))
# calculating average per year
dt.per.year$average_n_likes <- dt.per.year$n_likes / dt.per.year$n_users
# plotting
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) + geom_point()
# removing outliers for clarity purpouses
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) +
geom_point() +
coord_cartesian(xlim = c(-20, 60))
# removing outliers for clarity purpouses
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) +
geom_point() +
coord_cartesian(xlim = c(-20, 60)) +
+ geom_vline(xintercept=27)
# removing outliers for clarity purpouses
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) +
geom_point() +
coord_cartesian(xlim = c(-20, 60)) +
+ geom_vline(xintercept = 27, color = "red")
# removing outliers for clarity purpouses
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) +
geom_point() +
coord_cartesian(xlim = c(-20, 60)) +
geom_vline(xintercept = 27, color = "red")
# checking graphically the matches
#merging the number of matches with the user info data
dt.users.expanded <- merge(dt.users, dt.matches.per.user, by = c("user_id"))
#changing birth year to age
dt.users.expanded$age <- 2018 - dt.users.expanded$birth_year
#collapsing by year
dt.matches.per.year <- dt.users.expanded[, list(n_matches = sum(Freq)), by = "age"]
#plotting
ggplot(dt.matches.per.year, aes(x = age, y = n_matches)) + geom_point()
#collapsing by year
dt.matches.per.year <- dt.users.expanded[, list(n_matches = sum(Freq),
n_users = length(unique(user_id))),
by = "age"]
#calculating average
dt.matches.per.year$average <- dt.matches.per.year$n_matches / dt.matches.per.year$n_users
#plotting
ggplot(dt.matches.per.year, aes(x = age, y = average)) + geom_point()
# as above, removing outliers for a clearer understanding
ggplot(dt.matches.per.year, aes(x = age, y = n_matches)) +
geom_point() +
coord_cartesian(xlim = c(-20, 60)) +
geom_vline(xintercept = 27, color = "red")
# as above, removing outliers for a clearer understanding
ggplot(dt.matches.per.year, aes(x = age, y = average)) +
geom_point() +
coord_cartesian(xlim = c(-20, 60)) +
geom_vline(xintercept = 27, color = "red")
# removing outliers for clarity purpouses
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) +
geom_point() +
coord_cartesian(xlim = c(20, 60)) +
geom_vline(xintercept = 27, color = "red")
# as above, removing outliers for a clearer understanding
ggplot(dt.matches.per.year, aes(x = age, y = average)) +
geom_point() +
coord_cartesian(xlim = c(20, 60)) +
geom_vline(xintercept = 27, color = "red")
# removing outliers for clarity purpouses
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) +
geom_point() +
coord_cartesian(xlim = c(20, 60)) +
geom_vline(xintercept = 27, color = "red") +
geom_smooth(method = "lm", color = "green")
# removing outliers for clarity purpouses
ggplot(dt.per.year, aes(x = age_sender, y = average_n_likes)) +
geom_point() +
coord_cartesian(xlim = c(20, 60)) +
geom_vline(xintercept = 27, color = "red")
# as above, removing outliers for a clearer understanding
ggplot(dt.matches.per.year, aes(x = age, y = average)) +
geom_point() +
coord_cartesian(xlim = c(20, 60)) +
geom_vline(xintercept = 27, color = "red") +
geom_smooth(method = "lm")
View(dt.homosexual.like)
# adding a new line which will be be the couple ids
dt.homosexual.like$couple_id <- paste(dt.homosexual.like$sender_user_id,
dt.homosexual.like$receiver_user_id,
sep = "_and_")
#checking for duplicates
dt.homosexual.like[duplicated(couple_id)]
# creating a function that gets the gender based on the user id
get_gender <- function(user_id){
return(dt.users$gender[user_id])
}
# creating a function that gets the age based on the user id
get_age <- function(user_id){
return(2018 - dt.users$birth_year[user_id])
}
# using the above created functions to expand the likes database by adding gender and age of both sender and receiver
dt.likes$gender_sender <- vapply(dt.likes$sender_user_id,
get_gender,
character(1))
dt.likes$gender_receiver <- vapply(dt.likes$receiver_user_id,
get_gender,
character(1))
dt.likes$age_sender <- vapply(dt.likes$sender_user_id,
get_age,
numeric(1))
dt.likes$age_receiver <- vapply(dt.likes$receiver_user_id,
get_age,
numeric(1))
# adding a new variable that checks for same sex likes
dt.likes$same_sex <- dt.likes$gender_receiver == dt.likes$gender_sender
# creating a subset with only same sex likes
dt.homosexual.like <- dt.likes[same_sex == TRUE]
# number of same sex likes
nrow(dt.homosexual.like)
#summary of the graph
summary(g.likes)
# Doing the same for males
# collapsping per age
dt.male.test <- dt.male.attractive[, length(same_sex), by = "age_receiver"]
names(dt.male.test) <- c("age_receiver", "n_obs")
# ordering and printing
dt.male.test <- dt.male.test[order(-n_obs), ]
head(dt.male.test, 10)
# given the fact that invited are directioned and that there cannot be triads or cycles, the best and faster way to check for the cascades
length(cl.invites$csize)
shinyApp(ui=ui, server=server)
library(shiny)
shinyApp(ui=ui, server=server)
shinyApp ( u i  =  ui ,   server  =  server )
shinyApp ( ui  =  ui ,   server  =  server )
setwd("~/Downloads")
shinyApp ( ui  =  ui.R ,   server  =  server.R )
server <- function(input, output, session) {
# Filter the movies, returning a data frame
movies <- reactive({
# Due to dplyr issue #318, we need temp variables for input values
reviews <- input$reviews
oscars <- input$oscars
minyear <- input$year[1]
maxyear <- input$year[2]
minboxoffice <- input$boxoffice[1] * 1e6
maxboxoffice <- input$boxoffice[2] * 1e6
# Apply filters
m <- all_movies %>%
filter(
Reviews >= reviews,
Oscars >= oscars,
Year >= minyear,
Year <= maxyear,
BoxOffice >= minboxoffice,
BoxOffice <= maxboxoffice
) %>%
arrange(Oscars)
# Optional: filter by genre
if (input$genre != "All") {
genre <- paste0("%", input$genre, "%")
m <- m %>% filter(Genre %like% genre)
}
# Optional: filter by director
if (!is.null(input$director) && input$director != "") {
director <- paste0("%", input$director, "%")
m <- m %>% filter(Director %like% director)
}
# Optional: filter by cast member
if (!is.null(input$cast) && input$cast != "") {
cast <- paste0("%", input$cast, "%")
m <- m %>% filter(Cast %like% cast)
}
m <- as.data.frame(m)
# Add column which says whether the movie won any Oscars
# Be a little careful in case we have a zero-row data frame
m$has_oscar <- character(nrow(m))
m$has_oscar[m$Oscars == 0] <- "No"
m$has_oscar[m$Oscars >= 1] <- "Yes"
m
})
# Function for generating tooltip text
movie_tooltip <- function(x) {
if (is.null(x)) return(NULL)
if (is.null(x$ID)) return(NULL)
# Pick out the movie with this ID
all_movies <- isolate(movies())
movie <- all_movies[all_movies$ID == x$ID, ]
paste0("<b>", movie$Title, "</b><br>",
movie$Year, "<br>",
"$", format(movie$BoxOffice, big.mark = ",", scientific = FALSE)
)
}
# A reactive expression with the ggvis plot
vis <- reactive({
# Lables for axes
xvar_name <- names(axis_vars)[axis_vars == input$xvar]
yvar_name <- names(axis_vars)[axis_vars == input$yvar]
# Normally we could do something like props(x = ~BoxOffice, y = ~Reviews),
# but since the inputs are strings, we need to do a little more work.
xvar <- prop("x", as.symbol(input$xvar))
yvar <- prop("y", as.symbol(input$yvar))
movies %>%
ggvis(x = xvar, y = yvar) %>%
layer_points(size := 50, size.hover := 200,
fillOpacity := 0.2, fillOpacity.hover := 0.5,
stroke = ~has_oscar, key := ~ID) %>%
add_tooltip(movie_tooltip, "hover") %>%
add_axis("x", title = xvar_name) %>%
add_axis("y", title = yvar_name) %>%
add_legend("stroke", title = "Won Oscar", values = c("Yes", "No")) %>%
scale_nominal("stroke", domain = c("Yes", "No"),
range = c("orange", "#aaa")) %>%
set_options(width = 500, height = 500)
})
vis %>% bind_shiny("plot1")
output$n_movies <- renderText({ nrow(movies()) })
}
runApp()
install.packages(ggvis)
install.packages("ggvis")
runApp()
runApp()
runApp()
install.packages(c('shiny', 'ggvis', 'dplyr', 'RSQLite'))
install.packages(c("shiny", "ggvis", "dplyr", "RSQLite"))
shiny::runApp()
runApp('nda-assignment-similarity')
server <- function(input, output) {
output$stat.summary <- renderUI({
#create slider with renderUI
output$sidebar <- renderUI(
sliderInput("obs",
"Number of observations:",
min = 1,
max = 1000,
value = 500)
)
})
output$top.10 <- renderUI({
#create slider with renderUI
output$sidebar <- renderUI(
sliderInput("obs",
"Number of observations:",
min = 1,
max = 1000,
value = 500)
)
})
output$basic.network <- renderUI({
#create slider with renderUI
output$sidebar <- renderUI(
sliderInput("obs",
"Number of observations:",
min = 1,
max = 1000,
value = 500)
)
})
output$advanced.network <- renderUI({
#create slider with renderUI
output$sidebar <- renderUI(
sliderInput("obs",
"Number of observations:",
min = 1,
max = 1000,
value = 500)
)
})
}
ui <- fluidPage(
# Title
titlePanel("Group 12 - Spotify Top 10"),
# slider comes from the si object created in server.R
sidebarLayout(
sidebarPanel(
uiOutput("sidebar")
),
mainPanel(
tabsetPanel(
tabPanel("Summary of the Database", htmlOutput("stat.summary")) ,
tabPanel("Top 10 per date and country", htmlOutput("top.10")),
tabPanel("Network Analysis", htmlOutput("basic.network")),
tabPanel("Advanced Analysis", htmlOutput("advanced.network"))
)
)
)
)
I.Am.An.App.Name
Group 12 - Spotify Top 10
server
deployApp('Group 12 - Spotify Top 10')
install.packages('rsconnect')
library(rsconnect)
deployApp('Group 12 - Spotify Top 10')
deployApp('Group 12')
deployApp()
setwd("~/Desktop")
library(igraph)
library(igraph)
library(shiny)
library(data.table)
library(rsconnect)
setAccountInfo(name='laurens',
token='60B95632EA04F691ECBBABB5BDB8B88C',
secret='Fvoc/q/sSfl0iCY1WUlzN0IWztflCzcOljUIzPuc')
deployApp('I.Am.An.App.Name')
setwd("~/Desktop/Spotify app")
deployAPP()
deployApp()
ls()
wd
wd()
getwd()
deployApp()
deployApp()
